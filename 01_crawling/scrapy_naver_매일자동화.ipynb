{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 프로젝트 시작"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!scrapy startproject naver_news"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### items.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile naver_news/naver_news/items.py\n",
    "# %load naver_news/naver_news/items.py\n",
    "# Define here the models for your scraped items\n",
    "#\n",
    "# See documentation in:\n",
    "# https://docs.scrapy.org/en/latest/topics/items.html\n",
    "\n",
    "import scrapy\n",
    "\n",
    "\n",
    "class NaverNewsItem(scrapy.Item):\n",
    "    # define the fields for your item here like:\n",
    "    # name = scrapy.Field()\n",
    "    date = scrapy.Field()\n",
    "    press_agency = scrapy.Field()\n",
    "    category = scrapy.Field()\n",
    "    link = scrapy.Field()\n",
    "    title = scrapy.Field()\n",
    "    content = scrapy.Field()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### url 저장하는 함수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile naver_news/naver_news/spiders/naver_articles.py\n",
    "import requests\n",
    "import scrapy\n",
    "from scrapy.http import TextResponse\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "def get_urls(category='105'):\n",
    "    headers = {\n",
    "        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/86.0.4240.198 Safari/537.36'\n",
    "    }\n",
    "    date = (datetime.today() - timedelta(1)).strftime('%Y%m%d')\n",
    "    last_p, urls = '', []\n",
    "    for page in range(1, 1000, 10):\n",
    "        # 마지막 페이지로 \n",
    "        url = 'https://news.naver.com/main/list.nhn?mode=LSD&mid=sec&listType=title&sid1={}&date={}&page={}'.format(category, date, page)\n",
    "        req = requests.get(url, headers=headers)\n",
    "        resp = TextResponse(req.url, body=req.text, encoding='utf-8')\n",
    "\n",
    "        try:\n",
    "            chk_next = resp.xpath('//div[@class=\"paging\"]/a[@class=\"next nclicks(fls.page)\"]/text()')[0].extract()\n",
    "        except:\n",
    "            chk_next = '끝'\n",
    "\n",
    "        if chk_next == '끝':\n",
    "            # 마지막 페이지가 여러개일때\n",
    "            # 마지막 페이지가 1개일때\n",
    "            pages = resp.xpath('//a[@class=\"nclicks(fls.page)\"]/text()' |\n",
    "                              '//div[@class=\"paging\"]/strong').extract() \n",
    "            last_p = pages[-1]\n",
    "            print(last_p)\n",
    "            break\n",
    "\n",
    "    for page in range(1, int(last_p)+1):\n",
    "        urls.append('https://news.naver.com/main/list.nhn?mode=LSD&mid=sec&listType=title&sid1={}&date={}&page={}'.format(category, date, page))\n",
    "    return urls\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### spider.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile naver_news/naver_news/spiders/spider.py\n",
    "import time\n",
    "import re\n",
    "import requests\n",
    "import scrapy\n",
    "from .naver_articles import *\n",
    "from scrapy.http import TextResponse\n",
    "from naver_news.items import NaverNewsItem\n",
    "\n",
    "class NaverNewsSpider(scrapy.Spider):\n",
    "    name = 'naver_news'\n",
    "    allow_domain=[\"https://news.naver.com\"]\n",
    "    categ = {#'정치': '100',\n",
    "     #'101':'경제',\n",
    "     #'102': '사회',\n",
    "     '103': '생활/문화',\n",
    "     #'세계': '104',\n",
    "     '105': 'IT/과학'}\n",
    "    user_agent= 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/86.0.4240.198 Safari/537.36'\n",
    "    def start_requests(self):\n",
    "        all_category_urls = []\n",
    "        for category in self.categ.keys():\n",
    "            all_category_urls.append(get_urls(category))\n",
    "\n",
    "        for urls in all_category_urls:\n",
    "            for url in urls:\n",
    "                yield scrapy.Request(url, callback=self.parse)\n",
    "            time.sleep(10)\n",
    "            \n",
    "    def parse(self, resp):\n",
    "        links = resp.xpath('//*[@id=\"main_content\"]/div[2]/ul/li/a/@href').extract()\n",
    "        # links = [resp.urljoin(link) for link in links]\n",
    "        for link in links:\n",
    "            yield scrapy.Request(link, callback=self.parse_content)\n",
    "            \n",
    "    def parse_content(self, resp):\n",
    "        item = NaverNewsItem()\n",
    "        title = resp.xpath('//*[@id=\"articleTitle\"]/text() | //*[@id=\"content\"]/div[1]/div/h2/text()')[0].extract()\n",
    "        date = resp.xpath('//*[@id=\"main_content\"]/div[1]/div[3]/div/span[@class=\"t11\"]/text() | \\\n",
    "                        //div[@class=\"article_info\"]/span[@class=\"author\"]/em')[0].extract()\n",
    "        content = resp.xpath('//*[@id=\"articleBodyContents\"]/text() | \\\n",
    "                        //*[@id=\"articleBodyContents\"]/strong/text() | \\\n",
    "                        //*[@id=\"articleBodyContents\"]/div/text() | \\\n",
    "                        //*[@id=\"articleBodyContents\"]/div/div/text() | \\\n",
    "                        //*[@id=\"articleBodyContents\"]/font/text() | \\\n",
    "                        //*[@id=\"articleBodyContents\"]/div[2]/ul/li/span/span/text() | \\\n",
    "                        //*[@id=\"articeBody\"]/text()').extract()\n",
    "        content = [text.replace('\\xa0', ' ').strip() for text in content]\n",
    "        try:\n",
    "            c_num = resp.url.split('sid1=')[1].split('&')[0]\n",
    "        \n",
    "            item['date'] = re.findall('[0-9]{4}[.][0-9]{2}[.][0-9]{2}', date)[0]\n",
    "            item['category'] = self.categ[c_num]\n",
    "            item['press_agency'] = resp.xpath('//a[@class=\"nclicks(atp_press)\"]/img/@title | //div[@class=\"press_logo\"]/a/img/@alt')[0].extract()\n",
    "            item['link'] = resp.url\n",
    "            item['title'] = title.strip()\n",
    "            item['content'] = '\\n'.join(content).strip()\n",
    "\n",
    "            yield item\n",
    "        except:\n",
    "            print('nope') # url에 카테고리가 없는 연예기사 스포츠기사는 제외"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### mongo db 저장 함수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile naver_news/naver_news/mongodb.py\n",
    "import pymongo\n",
    "# DB와 연결\n",
    "client = pymongo.MongoClient('mongodb://127.0.0.1:27017/') \n",
    "# DB Table 지정\n",
    "db = client.news\n",
    "collection = db.articles"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### piplines.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile naver_news/naver_news/pipelines.py\n",
    "# %load naver_news/naver_news/pipelines.py\n",
    "# Define your item pipelines here\n",
    "#\n",
    "# Don't forget to add your pipeline to the ITEM_PIPELINES setting\n",
    "# See: https://docs.scrapy.org/en/latest/topics/item-pipeline.html\n",
    "\n",
    "\n",
    "# useful for handling different item types with a single interface\n",
    "from itemadapter import ItemAdapter\n",
    "from .mongodb import collection\n",
    "import time\n",
    "\n",
    "class NaverNewsPipeline:\n",
    "    def process_item(self, item, spider):\n",
    "        # time.sleep(10)\n",
    "        data = {\n",
    "            'p_date': item['date'],\n",
    "            'category': item['category'],\n",
    "            'press_agency': item['press_agency'],\n",
    "            'link': item['link'],\n",
    "            'title': item['title'],\n",
    "            'content': item['content'],\n",
    "            }\n",
    "        print('='*5)\n",
    "        collection.insert(data)\n",
    "        return item\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### settings.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile naver_news/naver_news/settings.py\n",
    "# %load naver_news/naver_news/settings.py\n",
    "# Scrapy settings for naver_news project\n",
    "#\n",
    "# For simplicity, this file contains only settings considered important or\n",
    "# commonly used. You can find more settings consulting the documentation:\n",
    "#\n",
    "#     https://docs.scrapy.org/en/latest/topics/settings.html\n",
    "#     https://docs.scrapy.org/en/latest/topics/downloader-middleware.html\n",
    "#     https://docs.scrapy.org/en/latest/topics/spider-middleware.html\n",
    "\n",
    "BOT_NAME = 'naver_news'\n",
    "\n",
    "SPIDER_MODULES = ['naver_news.spiders']\n",
    "NEWSPIDER_MODULE = 'naver_news.spiders'\n",
    "\n",
    "\n",
    "# Crawl responsibly by identifying yourself (and your website) on the user-agent\n",
    "#USER_AGENT = 'naver_news (+http://www.yourdomain.com)'\n",
    "\n",
    "# Obey robots.txt rules\n",
    "ROBOTSTXT_OBEY = False\n",
    "\n",
    "# Configure maximum concurrent requests performed by Scrapy (default: 16)\n",
    "#CONCURRENT_REQUESTS = 32\n",
    "\n",
    "# Configure a delay for requests for the same website (default: 0)\n",
    "# See https://docs.scrapy.org/en/latest/topics/settings.html#download-delay\n",
    "# See also autothrottle settings and docs\n",
    "#DOWNLOAD_DELAY = 3\n",
    "# The download delay setting will honor only one of:\n",
    "#CONCURRENT_REQUESTS_PER_DOMAIN = 16\n",
    "#CONCURRENT_REQUESTS_PER_IP = 16\n",
    "\n",
    "# Disable cookies (enabled by default)\n",
    "#COOKIES_ENABLED = False\n",
    "\n",
    "# Disable Telnet Console (enabled by default)\n",
    "#TELNETCONSOLE_ENABLED = False\n",
    "\n",
    "# Override the default request headers:\n",
    "#DEFAULT_REQUEST_HEADERS = {\n",
    "#   'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',\n",
    "#   'Accept-Language': 'en',\n",
    "#}\n",
    "\n",
    "# Enable or disable spider middlewares\n",
    "# See https://docs.scrapy.org/en/latest/topics/spider-middleware.html\n",
    "#SPIDER_MIDDLEWARES = {\n",
    "#    'naver_news.middlewares.NaverNewsSpiderMiddleware': 543,\n",
    "#}\n",
    "\n",
    "# Enable or disable downloader middlewares\n",
    "# See https://docs.scrapy.org/en/latest/topics/downloader-middleware.html\n",
    "#DOWNLOADER_MIDDLEWARES = {\n",
    "#    'naver_news.middlewares.NaverNewsDownloaderMiddleware': 543,\n",
    "#}\n",
    "\n",
    "# Enable or disable extensions\n",
    "# See https://docs.scrapy.org/en/latest/topics/extensions.html\n",
    "#EXTENSIONS = {\n",
    "#    'scrapy.extensions.telnet.TelnetConsole': None,\n",
    "#}\n",
    "\n",
    "# Configure item pipelines\n",
    "# See https://docs.scrapy.org/en/latest/topics/item-pipeline.html\n",
    "#ITEM_PIPELINES = {\n",
    "#    'naver_news.pipelines.NaverNewsPipeline': 300,\n",
    "#}\n",
    "\n",
    "# Enable and configure the AutoThrottle extension (disabled by default)\n",
    "# See https://docs.scrapy.org/en/latest/topics/autothrottle.html\n",
    "#AUTOTHROTTLE_ENABLED = True\n",
    "# The initial download delay\n",
    "#AUTOTHROTTLE_START_DELAY = 5\n",
    "# The maximum download delay to be set in case of high latencies\n",
    "#AUTOTHROTTLE_MAX_DELAY = 60\n",
    "# The average number of requests Scrapy should be sending in parallel to\n",
    "# each remote server\n",
    "#AUTOTHROTTLE_TARGET_CONCURRENCY = 1.0\n",
    "# Enable showing throttling stats for every response received:\n",
    "#AUTOTHROTTLE_DEBUG = False\n",
    "\n",
    "# Enable and configure HTTP caching (disabled by default)\n",
    "# See https://docs.scrapy.org/en/latest/topics/downloader-middleware.html#httpcache-middleware-settings\n",
    "#HTTPCACHE_ENABLED = True\n",
    "#HTTPCACHE_EXPIRATION_SECS = 0\n",
    "#HTTPCACHE_DIR = 'httpcache'\n",
    "#HTTPCACHE_IGNORE_HTTP_CODES = []\n",
    "#HTTPCACHE_STORAGE = 'scrapy.extensions.httpcache.FilesystemCacheStorage'\n",
    "ITEM_PIPELINES = {\n",
    "    'naver_news.pipelines.NaverNewsPipeline': 300,\n",
    "}"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
