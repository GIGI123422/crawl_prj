{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New Scrapy project 'naver_news_02', using template directory 'c:\\programdata\\anaconda3\\lib\\site-packages\\scrapy\\templates\\project', created in:\n",
      "    C:\\Code_dss15\\project\\02_CRAWLING\\naver_news_02\n",
      "\n",
      "You can start your first spider with:\n",
      "    cd naver_news_02\n",
      "    scrapy genspider example example.com\n"
     ]
    }
   ],
   "source": [
    "!scrapy startproject naver_news_02"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting naver_news_02/naver_news_02/items.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile naver_news_02/naver_news_02/items.py\n",
    "# %load naver_news_02/naver_news_02/items.py\n",
    "# Define here the models for your scraped items\n",
    "#\n",
    "# See documentation in:\n",
    "# https://docs.scrapy.org/en/latest/topics/items.html\n",
    "\n",
    "import scrapy\n",
    "\n",
    "\n",
    "class NaverNews02Item(scrapy.Item):\n",
    "    # define the fields for your item here like:\n",
    "    # name = scrapy.Field()\n",
    "    date = scrapy.Field()\n",
    "    press_agency = scrapy.Field()\n",
    "    category = scrapy.Field()\n",
    "    link = scrapy.Field()\n",
    "    title = scrapy.Field()\n",
    "    content = scrapy.Field()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__init__.py\n",
      "__pycache__\n",
      "article_url_1.csv\n",
      "article_url_2.csv\n",
      "article_url_3.csv\n",
      "article_url_4.csv\n",
      "article_url_half.csv\n",
      "article_urls.csv\n",
      "article_urls.xlsx\n",
      "data\n",
      "items.py\n",
      "middlewares.py\n",
      "mongodb.py\n",
      "pipelines.py\n",
      "settings.py\n",
      "spiders\n"
     ]
    }
   ],
   "source": [
    "!ls naver_news_02/naver_news_02/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting naver_news_02/naver_news_02/spiders/spider.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile naver_news_02/naver_news_02/spiders/spider.py\n",
    "# %load naver_news_02/naver_news_02/spiders/spider.py\n",
    "import pandas as pd\n",
    "import time\n",
    "import re\n",
    "import requests\n",
    "import scrapy\n",
    "from scrapy.http import TextResponse\n",
    "from naver_news_02.items import NaverNews02Item\n",
    "\n",
    "class NaverNews02Spider(scrapy.Spider):\n",
    "    name = 'naver_news_02'\n",
    "    allow_domain=[\"https://news.naver.com\"]\n",
    "    user_agent= 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/86.0.4240.198 Safari/537.36'\n",
    "    categ = {#'정치': '100',\n",
    "     '101':'경제',\n",
    "     '102': '사회',\n",
    "     '103': '생활/문화',\n",
    "     #'세계': '104',\n",
    "     '105': 'IT/과학'}\n",
    "    \n",
    "    def start_requests(self):\n",
    "        #df = pd.read_csv('article_url_1.csv')\n",
    "        df = pd.read_csv('naver_news_02/article_soci.csv')\n",
    "        rows = df.iloc\n",
    "        date_ex = '202011'\n",
    "        \n",
    "        for row in rows:\n",
    "            date_ = str(row['date'])\n",
    "            date_ = str(date_)[0:7]\n",
    "            if date_ != date_ex:\n",
    "                time.sleep(2)\n",
    "            #print(row['categ'], row['date'], row['last_p'])\n",
    "            for page in range(1, int(row['last_p'])+1):\n",
    "                url = 'https://news.naver.com/main/list.nhn?mode=LSD&mid=sec&listType=title&sid1={}&date={}&page={}'.format(row['categ'], row['date'], page)\n",
    "                yield scrapy.Request(url, callback=self.parse)\n",
    "            date_ex = str(date_)[0:7]\n",
    "            \n",
    "    def parse(self, resp):\n",
    "        links = resp.xpath('//*[@id=\"main_content\"]/div[2]/ul/li/a/@href').extract()\n",
    "        \n",
    "        # links = [resp.urljoin(link) for link in links]\n",
    "        for link in links:\n",
    "            yield scrapy.Request(link, callback=self.parse_content)\n",
    "            \n",
    "    def parse_content(self, resp):\n",
    "        item = NaverNews02Item()\n",
    "        title = resp.xpath('//*[@id=\"articleTitle\"]/text() | //*[@id=\"content\"]/div[1]/div/h2/text() | \\\n",
    "            //h4[@class=\"title\"]/text()')[0].extract()\n",
    "        date = resp.xpath('//*[@id=\"main_content\"]/div[1]/div[3]/div/span[@class=\"t11\"]/text() | \\\n",
    "                        //div[@class=\"article_info\"]/span[@class=\"author\"]/em/text()|\\\n",
    "                        //div[@class=\"info\"]/span[1]/text()')[0].extract()\n",
    "        content = resp.xpath('//*[@id=\"articleBodyContents\"]/text() | \\\n",
    "                        //*[@id=\"articleBodyContents\"]/strong/text() | \\\n",
    "                        //*[@id=\"articleBodyContents\"]/div/text() | \\\n",
    "                        //*[@id=\"articleBodyContents\"]/div/div/text() | \\\n",
    "                        //*[@id=\"articleBodyContents\"]/font/text() | \\\n",
    "                        //*[@id=\"articleBodyContents\"]/div[2]/ul/li/span/span/text() | \\\n",
    "                        //*[@id=\"newsEndContents\"]/text() | \\\n",
    "                        //*[@id=\"articeBody\"]/text()').extract()\n",
    "        content = [text.replace('\\xa0', ' ').strip() for text in content]\n",
    "        categ_num = resp.url.split('sid1=')[1].split('&')[0]\n",
    "        \n",
    "        item['date'] = re.findall('[0-9]{4}[.][0-9]{2}[.][0-9]{2}', date)[0]\n",
    "        item['category'] = self.categ[categ_num]\n",
    "        item['press_agency'] = resp.xpath('//a[@class=\"nclicks(atp_press)\"]/img/@title | //div[@class=\"press_logo\"]/a/img/@alt | \\\n",
    "                                          //*[@id=\"pressLogo\"]/a/img/@alt')[0].extract()\n",
    "        item['link'] = resp.url\n",
    "        item['title'] = title.strip()\n",
    "        item['content'] = '\\n'.join(content).strip()\n",
    "        \n",
    "        yield item"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting naver_news_02/naver_news_02/pipelines.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile naver_news_02/naver_news_02/pipelines.py\n",
    "# %load naver_news_02/naver_news_02/pipelines.py\n",
    "# Define your item pipelines here\n",
    "#\n",
    "# Don't forget to add your pipeline to the ITEM_PIPELINES setting\n",
    "# See: https://docs.scrapy.org/en/latest/topics/item-pipeline.html\n",
    "\n",
    "\n",
    "# useful for handling different item types with a single interface\n",
    "from itemadapter import ItemAdapter\n",
    "from .mongodb import collection\n",
    "\n",
    "class NaverNews02Pipeline:\n",
    "    def process_item(self, item, spider):\n",
    "        # time.sleep(10)\n",
    "        data = {\n",
    "            'p_date': item['date'],\n",
    "            'category': item['category'],\n",
    "            'press_agency': item['press_agency'],\n",
    "            'link': item['link'],\n",
    "            'title': item['title'],\n",
    "            'content': item['content'],\n",
    "            }\n",
    "        print('='*5)\n",
    "        collection.insert(data)\n",
    "        return item\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting naver_news_02/naver_news_02/mongodb.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile naver_news_02/naver_news_02/mongodb.py\n",
    "import pymongo\n",
    "# DB와 연결\n",
    "client = pymongo.MongoClient('mongodb://127.0.0.1:27017/') \n",
    "# DB Table 지정\n",
    "db = client.news\n",
    "collection = db.articles_society"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting naver_news_02/naver_news_02/settings.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile naver_news_02/naver_news_02/settings.py\n",
    "# %load naver_news_02/naver_news_02/settings.py\n",
    "# Scrapy settings for naver_news_02 project\n",
    "#\n",
    "# For simplicity, this file contains only settings considered important or\n",
    "# commonly used. You can find more settings consulting the documentation:\n",
    "#\n",
    "#     https://docs.scrapy.org/en/latest/topics/settings.html\n",
    "#     https://docs.scrapy.org/en/latest/topics/downloader-middleware.html\n",
    "#     https://docs.scrapy.org/en/latest/topics/spider-middleware.html\n",
    "\n",
    "BOT_NAME = 'naver_news_02'\n",
    "\n",
    "SPIDER_MODULES = ['naver_news_02.spiders']\n",
    "NEWSPIDER_MODULE = 'naver_news_02.spiders'\n",
    "\n",
    "\n",
    "# Crawl responsibly by identifying yourself (and your website) on the user-agent\n",
    "#USER_AGENT = 'naver_news_02 (+http://www.yourdomain.com)'\n",
    "\n",
    "# Obey robots.txt rules\n",
    "ROBOTSTXT_OBEY = False\n",
    "\n",
    "# Configure maximum concurrent requests performed by Scrapy (default: 16)\n",
    "#CONCURRENT_REQUESTS = 32\n",
    "\n",
    "# Configure a delay for requests for the same website (default: 0)\n",
    "# See https://docs.scrapy.org/en/latest/topics/settings.html#download-delay\n",
    "# See also autothrottle settings and docs\n",
    "#DOWNLOAD_DELAY = 3\n",
    "# The download delay setting will honor only one of:\n",
    "#CONCURRENT_REQUESTS_PER_DOMAIN = 16\n",
    "#CONCURRENT_REQUESTS_PER_IP = 16\n",
    "\n",
    "# Disable cookies (enabled by default)\n",
    "#COOKIES_ENABLED = False\n",
    "\n",
    "# Disable Telnet Console (enabled by default)\n",
    "#TELNETCONSOLE_ENABLED = False\n",
    "\n",
    "# Override the default request headers:\n",
    "#DEFAULT_REQUEST_HEADERS = {\n",
    "#   'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',\n",
    "#   'Accept-Language': 'en',\n",
    "#}\n",
    "\n",
    "# Enable or disable spider middlewares\n",
    "# See https://docs.scrapy.org/en/latest/topics/spider-middleware.html\n",
    "#SPIDER_MIDDLEWARES = {\n",
    "#    'naver_news_02.middlewares.NaverNews02SpiderMiddleware': 543,\n",
    "#}\n",
    "\n",
    "# Enable or disable downloader middlewares\n",
    "# See https://docs.scrapy.org/en/latest/topics/downloader-middleware.html\n",
    "#DOWNLOADER_MIDDLEWARES = {\n",
    "#    'naver_news_02.middlewares.NaverNews02DownloaderMiddleware': 543,\n",
    "#}\n",
    "\n",
    "# Enable or disable extensions\n",
    "# See https://docs.scrapy.org/en/latest/topics/extensions.html\n",
    "#EXTENSIONS = {\n",
    "#    'scrapy.extensions.telnet.TelnetConsole': None,\n",
    "#}\n",
    "\n",
    "# Configure item pipelines\n",
    "# See https://docs.scrapy.org/en/latest/topics/item-pipeline.html\n",
    "#ITEM_PIPELINES = {\n",
    "#    'naver_news_02.pipelines.NaverNews02Pipeline': 300,\n",
    "#}\n",
    "\n",
    "# Enable and configure the AutoThrottle extension (disabled by default)\n",
    "# See https://docs.scrapy.org/en/latest/topics/autothrottle.html\n",
    "#AUTOTHROTTLE_ENABLED = True\n",
    "# The initial download delay\n",
    "#AUTOTHROTTLE_START_DELAY = 5\n",
    "# The maximum download delay to be set in case of high latencies\n",
    "#AUTOTHROTTLE_MAX_DELAY = 60\n",
    "# The average number of requests Scrapy should be sending in parallel to\n",
    "# each remote server\n",
    "#AUTOTHROTTLE_TARGET_CONCURRENCY = 1.0\n",
    "# Enable showing throttling stats for every response received:\n",
    "#AUTOTHROTTLE_DEBUG = False\n",
    "\n",
    "# Enable and configure HTTP caching (disabled by default)\n",
    "# See https://docs.scrapy.org/en/latest/topics/downloader-middleware.html#httpcache-middleware-settings\n",
    "#HTTPCACHE_ENABLED = True\n",
    "#HTTPCACHE_EXPIRATION_SECS = 0\n",
    "#HTTPCACHE_DIR = 'httpcache'\n",
    "#HTTPCACHE_IGNORE_HTTP_CODES = []\n",
    "#HTTPCACHE_STORAGE = 'scrapy.extensions.httpcache.FilesystemCacheStorage'\n",
    "ITEM_PIPELINES = {\n",
    "    'naver_news_02.pipelines.NaverNews02Pipeline': 300,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
